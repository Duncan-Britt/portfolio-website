const portfolioArticles = [{"dateStr":"11/14/2021","html":"<div>\n<a class=\"portfolio_link\" href=\"#word_ladders_writeup\">Software-Generated Word Ladder Puzzles & Web App</a><time datetime=\"2022-01-12\">01/12/2022</time>\n<article id=\"word_ladders_writeup\" loading=\"lazy\">\n<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://word-ladders.herokuapp.com/\">Site</a>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/Duncan-Britt/word_ladders#readme\">Repo</a>\n<br>\n<br>\n\n<h2 id=\"overview\">Overview</h2>\n<p>This web app generates word ladders and asks the user to solve them within a maximum number of steps. The puzzles are generated by the server in response to a request and stored in session data. At a high level, the word ladder puzzles are generated by</p>\n<ol>\n<li>making a random word ladder using the <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/filiph/english_words\">5,000 most commonly used English words</a></li>\n<li>finding the shortest path between those first and last words in the ladder using only the 5000 most common English words.</li>\n</ol>\n<p>The length of this shortest path determines the maximum number of steps for the user to solve the puzzle.</p>\n<h4 id=\"additional-features\">Additional Features</h4>\n<ul>\n<li>Signup, Login/Logout, Edit/Delete Account</li>\n<li>History of a user&#39;s previous solutions</li>\n<li>Leaderboard</li>\n</ul>\n<br>\n\n<h2 id=\"technologies-used\">Technologies Used</h2>\n<p>The backend for the Web App is written in Ruby and uses <a href=\"http://sinatrarb.com/intro.html\">Sinatra</a>, <a href=\"https://ruby-doc.org/stdlib-2.7.1/libdoc/erb/rdoc/ERB.html\">ERB</a>, and a PostgreSQL database.</p>\n<br>\n\n<h2 id=\"gameplay\">Gameplay</h2>\n<p>User input steps are considered valid if</p>\n<ol>\n<li>there is one letter of difference between the step and the previous step</li>\n<li>and the word can be found among the <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/cmusphinx/cmudict\">200,000 most commonly used English words</a></li>\n</ol>\n<h4 id=\"examples\">Examples:</h4>\n<ul>\n<li>buy =&gt; buys: valid</li>\n<li>buy =&gt; bus: valid</li>\n<li>buy =&gt; busy: valid</li>\n<li>busy =&gt; buoy: valid</li>\n<li>has =&gt; as: valid</li>\n<li>use =&gt; bus: invalid</li>\n<li>abuse =&gt; bus: invalid</li>\n</ul>\n<br>\n\n<h2 id=\"generating-puzzles\">Generating Puzzles</h2>\n<p>I parsed the data from a csv file of 5,000 most commonly used English words into a graphical representation of the data, which I then serialized to a YAML file so that my ruby code can more efficiently recreate it. Each vertex in the graph represents a word. The neighbors of every vertex in the graph are the vertices whose word is &quot;adjacent&quot; to the word stored by the vertex.</p>\n<figure>\n\n<p><img src=\"public/images/graph.svg\" alt=\"diagram\"></p>\n</figure>\n<figcaption>\"The\" and its neighbors.</figcaption>\n\n<p>Among my 5k words, it is possible find two words between which there simply is no path. So inorder to not waste time solving for the shortest path between two words for whom no path exists, my program first creates a ladder randomly.</p>\n<p>Then, my program takes the first and last words from the random ladder, ignores the rest, and attempts to find the shortest path between them using a breadth first search of the graph. If it takes too long, (if the queue is getting too long) it will inform the calling method which will try again with an entirely new random word ladder.</p>\n<textarea data-lang=\"ruby\">\nclass Vertex\n  #...\n  def traverse(end_point)\n    return [self.data, end_point.data] if self.neighbors.include? end_point\n\n    queue = [[self]]\n    while (queue.length != 0)\n      if queue.length > 1_000_000\n        return :timeout\n      end\n      path = queue.shift\n      node = path[-1]\n      return path.map { |n| n.data } if node == end_point\n\n      node.neighbors.each do |neighbor|\n        next if path.include?(neighbor)\n        new_path = path.dup\n        new_path << neighbor\n        queue.push(new_path)\n      end\n    end\n  end\n  #...\nend\n</textarea>\n<br>\n<br>\n\n<h2 id=\"open-issues\">Open Issues</h2>\n<p>I decided to use 5,000 words after some amount of trial and error. With larger word sets, it takes longer and longer to deserialize the graph. I would like it be able to make use of a broader vocabulary, and I have some ideas for how to make that happen. Although there are probably a variety of ways I could improve the speed of generation of puzzles (multithreading, for instance), I think the best solution is to make it a non-issue by caching previously generated word ladders and reusing them. The reason I haven&#39;t done that already basically comes down to the fact that I wanted to get the app up and running, and caching previously generated word ladders introduces some complexity I didn&#39;t want to deal with just yet. But that would be the next step.</p>\n<p>Caching previously generated puzzles opens up a number of possibilities such as:</p>\n<ul>\n<li>displaying other user&#39;s solutions for the same puzzle</li>\n<li>ranking puzzles based on quality/difficulty</li>\n<li>Algorithmically determining which puzzle to show the user</li>\n<li>Massively broadening the vocabulary of the word set</li>\n</ul>\n<p>I have some plans to make the puzzles more conceptually interesting by making them out of conceptually related words rather than just random, but that requires a greater vocabulary first.</p>\n<p>Another issue is that the leaderboard is too simplistic. It only measures the number of puzzles a user has solved. There&#39;s no extra credit for getting a puzzle in fewer steps, solving a puzzler faster, solving a harder puzzle, etc....</p>\n<br>\n\n</article>\n</div>"},{"dateStr":"01/19/2022","html":"<div>\n<a class=\"portfolio_link\" href=\"#splice_writeup\">Splice: A templating language for the front end</a><time datetime=\"2022-01-19\">01/19/2022</time>\n<article id=\"splice_writeup\" loading=\"lazy\">\n<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://duncan-britt.github.io/splice-docs\">Docs</a>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/Duncan-Britt/Splice-Lang\">Repo</a>\n<br>\n<br>\n\n<h2 id=\"overview\">Overview</h2>\n<p>Splice supports nesting, partial templates, iterators, conditionals, variable assignment, local scope, comments, escape characters, and escapes HTML by default. In its minified form, it is only 3.06 KB uncompressed, and has no dependencies.</p>\n<p>I&#39;ve written extensively about <em>how to use</em> Splice, and that writing is available on the <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://duncan-britt.github.io/splice-docs\">documentation website.</a> I will use this space to talk about the implementation of the language, design process/choices, challenges, and open issues.</p>\n<br>\n\n<h2 id=\"abstract-syntax-tree-ast\">Abstract Syntax Tree (AST)</h2>\n<p>The AST for Splice begins not with a single root node, but with a collection of its children - the root node is imaginary. There are three types of nodes in the AST:</p>\n<ul>\n<li>text</li>\n<li>binding</li>\n<li>operator</li>\n</ul>\n<p>Text nodes and binding nodes are leaf nodes, whereas operator nodes have a body property which is a subtree. (I think of the body as just a special kind of argument that always comes last). For example, the AST for the following splice template:</p>\n<textarea data-lang=\"\">\n<p>Hello, (: username :)! Here are your todos:</p>\n<ul>\n  (:~ each todos as 'todo {\n    <li>\n      <h2>(: todo.name :)</h2>\n      <p>(: todo.description :)</p>\n    </li>\n  }:)\n</ul>\n</textarea>\n<br>\n\n<p>would look something like this:</p>\n<figure>\n\n<p><img src=\"public/images/ast.svg\" alt=\"diagram\"></p>\n</figure>\n<figcaption>An example Splice AST</figcaption>\n<br>\n\n<h2 id=\"lexer--parser\">Lexer &amp; Parser</h2>\n<p>The lexer and parser work together to turn the former template into the latter AST. The job of a lexer is to split up the text of a program into relevant tokens, and the job of a parser is to create the AST from those tokens. In my program, these roles are intertwined- tokens are parsed as soon as they are identified. My lexing/parsing function takes the template and returns an AST. Within, it runs a loop which invokes a function that identifies a single token, parses it, and returns the remaining template.</p>\n<textarea data-lang=\"javascript\">\n// parse :: String -> Array{Object}\nfunction parse(template) {\n  const ast = [];\n\n  while (template) {\n    template = parseToken(template, ast);\n  }\n\n  return ast;\n}\n</textarea>\n<br>\n\n<p>When an operator body token is identified, the parser simply creates an AST from the operator body token by recursively invoking the main lexing/parsing function. The recursive nature of the lexer/parser reflects the recursive nature of the syntax tree- operator bodies are subtrees.</p>\n<textarea data-lang=\"javascript\">\n// parseFunction :: String -> Array{String, Object}\nfunction parseFunction(template) {\n  //... code omitted for brevity\n  [ token, bodyAST ] = parseBody(template);\n  //...\n  const expr = {\n    type: 'op',\n    name: op,\n    args: args,\n    body: bodyAST,\n  };\n  //...\n}\n\n// parseBody :: String -> Array{String, Array{Object}}\nfunction parseBody(template) {\n  //...\n  return [resultToken, parse(body)];\n}\n</textarea>\n\n<br>\n\n<h2 id=\"generator\">Generator</h2>\n<p>Now the hard work is done! All thats left is to turn the tree into text again. A reminder, when someone uses the compiler, they pass an object with the data to be filled into the template. That object can be thought of as the global scope for the Splice program. The job of the generator is to use the scope and the AST to render the final text output.</p>\n<p>The generator has knowledge of the three node types. For operator nodes, it must return the result of invoking the associated function, as all valid operators in the language have an internal function associated with them.</p>\n<p>Valid bindings refer to some property of the scope object, so all generator has to do is look up that property for a given binding node and return it. And of course, for text nodes, just return text!</p>\n<textarea data-lang=\"javascript\">\n// evaluateAll :: Array, Object -> String\nfunction evaluateAll(ast, scope) {\n  return ast.reduce((html, expr) => html + evaluate(expr, scope), \"\");\n}\n\n// evaluate :: Object, Object -> String\nfunction evaluate(expr, scope) {\n  switch (expr.type) {\n    case \"op\":\n      return templateFns[expr.name](scope, ...expr.args, expr.body);\n    case \"binding\":\n      let value = expr.chain.reduce((data, prop) => data[prop], scope[expr.name]);\n      if (typeof value == 'string') {\n        return expr.escape ? escapeHTML(value) : value;\n      }\n\n      return value;\n    case 'text':\n      return escapeChars(expr.value);\n  }\n}\n</textarea>\n<br>\n\n<h2 id=\"my-process\">My Process</h2>\n<p>My original design for the syntax didn&#39;t stick. It looked like this:</p>\n<textarea data-lang=\"\">\n<<~ each madlibs >>\n  <p>The << $.adjective >> << $.noun >> << $.verb >></p>\n<< end >>\n</textarea>\n<br>\n\n<p>I got as far as making a prototype documentation site but decided I didn&#39;t love the amount of angle brackets. It wasn&#39;t much trouble to change the syntax, but even so, it goes to show that it&#39;s worthwhile to make sure you&#39;re solving the right problem before you start coding.</p>\n<p>For the implementation, I started by hardcoding a prototype AST as well as a test &#39;scope&#39;- a set of data with which to test the compiler. Then I went on to writing the generator of the language. I think this was wise because it allowed me to easily modify the AST to suit the needs of the generator when I discovered something needed to change, without having to rewrite components of the parser.</p>\n<br>\n\n<h2 id=\"challenges\">Challenges</h2>\n<p>After creating the documentation website using Splice and thinking that it was good to go, I ventured to try opening the website in Safari, and lo and behold, the site crashed immediately. I learned the hard way that not all browsers support look-behinds in regular expressions, because I had <strong>49</strong> of them in my Splice compiler.</p>\n<p>The need for these look-behinds in my regular expressions was to avoid my escape character: <code>\\</code>. This meant that I had to replace much of the logic for tokenization without the benefit of regular expressions. On the bright side, I think the readability of my code has benefited significantly from this change.</p>\n<p>To do this, I wrote a helper function to do most of the heavy lifting:</p>\n<textarea data-lang=\"javascript\">\n// strTok :: String, String -> String, String\nfunction strTok(text, endChars, chop = false) {\n  let i = 0;\n  let j = endChars.length;\n  while (j <= text.length) {\n    if (text.slice(i, j) == endChars && text.slice(i-1, j) != '\\\\' + endChars) {\n      if (chop) {\n        return [text.slice(0, i), text.slice(i + endChars.length)];\n      } else {\n        return [text.slice(0, i), text.slice(i)];\n      }\n    }\n    i++;\n    j++;\n  }\n  return [text, ''];\n}\n</textarea>\n<br>\n\n<p>To get a token and the remaining template it can be used like this:</p>\n<textarea data-lang=\"javascript\">\n[ token, template ] = strTok(template, '(:');\n</textarea>\n<br>\n\n<h2 id=\"open-issues\">Open issues</h2>\n<p>It would be good to enable pre-compilation of the templates to enable faster performance. My idea is to have an executable that will parse the template(s) within an html file and compile the syntax tree into javascript code, and then write it to a file. The resulting js file would contain a function which contains the syntax tree written as object/array/string literals. That way there is no parsing client-side, only compilation to html.</p>\n<p>Some other things to work on:</p>\n<ul>\n<li>improving the error messages for the language</li>\n<li>writing more extensive tests</li>\n<li>enabling a user to register a helper function</li>\n<li>enable syntax highlighting for the atom text editor</li>\n</ul>\n<br>\n\n</article>\n</div>"}]